{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Wee learn word embeddings of words from the song lyrics using skip-gram word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils.utils as utils\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data and do some cleanup.\n",
    "\n",
    "doc = utils.load_data(filepath='data/mj_lyrics.txt')\n",
    "\n",
    "doc = re.sub('\\n+','\\n ',doc)\n",
    "doc = re.sub(' +',' ', doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants for network\n",
    "\n",
    "batch_size = 256\n",
    "embedding_dims = 64\n",
    "neg_smaples = 32\n",
    "epochs = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create word2index and index2word mappings\n",
    "word2idx, idx2word, vocab_size = utils.word_idx_mappings(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SkipGram pairs  with context = 1\n",
    "\n",
    "skip_gram_pairs=[]\n",
    "\n",
    "words = doc.lower().split()\n",
    "\n",
    "for i in range(1, len(words)-1):\n",
    "    word_context_pair = [[word2idx[words[i-1]],\n",
    "                         word2idx[words[i+1]]],\n",
    "                         word2idx[words[i]]]\n",
    "    \n",
    "    skip_gram_pairs += [[word_context_pair[1],word_context_pair[0][0]]]\n",
    "    skip_gram_pairs += [[word_context_pair[1],word_context_pair[0][1]]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skipgram_batch(batch_size):\n",
    "    indices = list(range(len(skip_gram_pairs)))\n",
    "    ix = 0\n",
    "    np.random.shuffle(indices)\n",
    "    batch = indices[:batch_size]\n",
    "    x = [skip_gram_pairs[i][0] for i in batch]\n",
    "    y = [skip_gram_pairs[i][1] for i in batch]\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x,y[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = 'word2vec'\n",
    "    \n",
    "    def build(self,embedding_dims, neg_samples):\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            self._inputs = tf.placeholder(tf.int32, shape=[batch_size], name='inputs')\n",
    "            self._labels = tf.placeholder(tf.int32, shape = [batch_size,1], name='labels')\n",
    "            \n",
    "            \n",
    "            with tf.name_scope('embeddings'):\n",
    "                self.embeddings = tf.get_variable(dtype=tf.float32, initializer=tf.random_uniform(shape=[vocab_size,embedding_dims],minval=-1.0,maxval=1.0), name='embed_matrix')\n",
    "                \n",
    "                #lookup table\n",
    "                self.embed = tf.nn.embedding_lookup(self.embeddings, self._inputs)\n",
    "                \n",
    "            with tf.name_scope('Loss'):\n",
    "                \n",
    "                self.nce_weights = tf.get_variable(dtype=tf.float32, shape=[vocab_size,embedding_dims], name='nce_weignts', initializer=tf.random_normal_initializer())\n",
    "                \n",
    "                self.nce_biases = tf.get_variable(dtype=tf.float32, shape=[vocab_size], name='nce_biases', initializer=tf.random_normal_initializer())\n",
    "                \n",
    "                self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=self.nce_weights, biases=self.nce_biases,inputs=self.embed,labels=self._labels, num_sampled=neg_smaples, num_classes=vocab_size))\n",
    "                \n",
    "                global_step = tf.Variable(0, trainable=False)\n",
    "                \n",
    "                learning_rate = tf.train.exponential_decay(learning_rate=1e-2,\n",
    "                                                          global_step=global_step,decay_steps=1000,decay_rate=0.95,staircase=True)\n",
    "                \n",
    "                self.train_step = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "                \n",
    "    def train(self,epochs):\n",
    "        \n",
    "        with tf.Session() as s:\n",
    "            \n",
    "            s.run(tf.global_variables_initializer())\n",
    "            \n",
    "            steps = len(skip_gram_pairs) // batch_size\n",
    "                        \n",
    "            for e in range(epochs):\n",
    "                print(f'Epoch {e+1}:')\n",
    "                for step in range(steps):\n",
    "                    X_batch, y_batch = get_skipgram_batch(batch_size)\n",
    "                    \n",
    "                    _ = s.run([self.train_step], feed_dict={self._inputs: X_batch, self._labels: y_batch})\n",
    "                    \n",
    "                    if step % 100 == 0:\n",
    "                        tr_loss = s.run([self.loss], feed_dict={self._inputs: X_batch, self._labels: y_batch})\n",
    "                        print(f'Step {step}, LOSS: {tr_loss}')\n",
    "            \n",
    "            #Normalize embeddings here\n",
    "            \n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(self.embeddings),1,keepdims=True))\n",
    "            norm_embeddings = self.embeddings / norm\n",
    "            \n",
    "            norm_embeddings_matrix = s.run(norm_embeddings)\n",
    "            \n",
    "            return norm_embeddings_matrix\n",
    "                    \n",
    "                    \n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec()\n",
    "\n",
    "w2v.build(embedding_dims,neg_smaples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Step 0, LOSS: [138.04443]\n",
      "Step 100, LOSS: [111.20104]\n",
      "Step 200, LOSS: [77.61796]\n",
      "Step 300, LOSS: [51.187477]\n",
      "Epoch 2:\n",
      "Step 0, LOSS: [65.94971]\n",
      "Step 100, LOSS: [44.89686]\n",
      "Step 200, LOSS: [71.28148]\n",
      "Step 300, LOSS: [21.60888]\n",
      "Epoch 3:\n",
      "Step 0, LOSS: [39.668777]\n",
      "Step 100, LOSS: [45.002]\n",
      "Step 200, LOSS: [20.35807]\n",
      "Step 300, LOSS: [18.900162]\n",
      "Epoch 4:\n",
      "Step 0, LOSS: [22.980154]\n",
      "Step 100, LOSS: [5.3417177]\n",
      "Step 200, LOSS: [16.045723]\n",
      "Step 300, LOSS: [29.083668]\n",
      "Epoch 5:\n",
      "Step 0, LOSS: [7.6336718]\n",
      "Step 100, LOSS: [6.8079696]\n",
      "Step 200, LOSS: [5.772628]\n",
      "Step 300, LOSS: [13.515457]\n",
      "Epoch 6:\n",
      "Step 0, LOSS: [21.24993]\n",
      "Step 100, LOSS: [3.1834276]\n",
      "Step 200, LOSS: [4.573825]\n",
      "Step 300, LOSS: [3.5942945]\n",
      "Epoch 7:\n",
      "Step 0, LOSS: [6.5755787]\n",
      "Step 100, LOSS: [3.9641469]\n",
      "Step 200, LOSS: [21.544903]\n",
      "Step 300, LOSS: [2.9790657]\n",
      "Epoch 8:\n",
      "Step 0, LOSS: [3.9184122]\n",
      "Step 100, LOSS: [3.078453]\n",
      "Step 200, LOSS: [2.7673883]\n",
      "Step 300, LOSS: [2.9712257]\n",
      "Epoch 9:\n",
      "Step 0, LOSS: [9.139435]\n",
      "Step 100, LOSS: [2.8860111]\n",
      "Step 200, LOSS: [2.6749275]\n",
      "Step 300, LOSS: [6.502402]\n",
      "Epoch 10:\n",
      "Step 0, LOSS: [3.818266]\n",
      "Step 100, LOSS: [2.5118449]\n",
      "Step 200, LOSS: [2.3727822]\n",
      "Step 300, LOSS: [3.908939]\n"
     ]
    }
   ],
   "source": [
    "em = w2v.train(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh\n",
      "0.8336346\n",
      "thought\n",
      "0.8287991\n",
      "eyes\n",
      "0.82837164\n",
      "got\n",
      "0.82503355\n",
      "with\n",
      "0.82464486\n",
      "rhythm\n",
      "0.8235712\n",
      "a\n",
      "0.8229039\n",
      "baby,\n",
      "0.81785923\n",
      "devil\n",
      "0.816272\n"
     ]
    }
   ],
   "source": [
    "ref_word = em[word2idx['childhood']]\n",
    "\n",
    "cosine_dists = np.dot(em,ref_word)\n",
    "\n",
    "wrds = np.argsort(cosine_dists)[::-1][1:10]\n",
    "\n",
    "for w in wrds:\n",
    "    print(idx2word[w])\n",
    "    print(cosine_dists[w])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(open('data/word2vec/mj_lyrics_w2v.npz', 'wb'),em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/word2vec/word2idx.pkl','wb') as handle:\n",
    "    pickle.dump(word2idx,handle)\n",
    "with open('data/word2vec/idx2word.pkl','wb') as handle:\n",
    "    pickle.dump(idx2word,handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
